Problém hledání cesty; navigační pravidla, reprezentace terénu
- obecný termín pro schopnost chápat, reason about, ... prostředí ve kterém se agent nachází 
    - navigace, vztahy objektů, ...
    - odpovídá na mnoho otázek: kde jsem, jak uhnout raketě, kudy se dostat k X, ...

- pokud neznáme prostředí: 
    - nějak náhodně, např. mars rovers (skrze reaktivní programování)
        - jdi náhodně, pokladej radioaktivni blbost, pokladej radioaktivni smeti, když narazíš na radioaktivni smeti jdi jinam, ... 
    - bug algoritmy: označ hitpoint a leavepoint, objížděj pokud narazíš na překážku, dokud nenarazíš na průsečík přímky leavepoint<>hitpoint

- reprezentace terénu: 
    - visibility matrix
    - height map 
    - plná 3D polygon reprezentace
        - moc jemné na jekékoliv počítání 
        - užitečné na fine raycasting pro např. "mohu vidět XYZ pokud vím, že bych nejspíš mohl"
        - může být optimalizované skrze BSP tree (axis aligned splitting, ...)

    - navgraph 
        - navigation points, edges 
        - spoustu informací se ztratí -> přádání area informace (pro steering) -> skoro navmesh
    - navmesh
        - má objemy, lepší steering 
        - musí být coupled with navmesh 
        - získat contour, triangularizace, odstranit stěny, 
            - duální graf je ~funkční navgraph
            - většinou třeba ručně přidat relevantní navpointy (hiding spots, ...)
            - třeba přidat relevantní hrany: jumpoints, ...

        - těžko odhadnout jak má být jemný: 
    - voxel navigace: 
        - plně 3D

    - navmesh navigace  
        - různé polygony různě velké -> může vést k špatně vygenerovanému navgraphu -> vybereš delší cestu kvůli chybějící duální konektivítě 
            - https://drive.google.com/file/d/12VNtbCh9h8Dm6IbfJln8TbFEFob6g-by/view
        - refining 
            - použití středů polygonů nefunguje 
            - středů crossed-hran-mezi-polygony: funguje ale ne optimální 
            - funnel algoritmus. Jede po úhlopříčkách, najde docela pěknou krátkou cestu 

- prebaked waypointing
    - najdeme přímou cestu k nejbližšímu waypointu, A* v grafu 
    - waypointy mohou být too coarse
    - generičtější navmesh může být lepší 


- navigace velkých skupin jednotek  
    - proper pathfinding pro leadera skupiny, zbytek steering okolo něj
        - do not overlap units
        - start fast 
        - maintain coherence of the group 
        - solve dynamic obstacles 
        -> těžké 

    - různá řešení 
        - relaxace do grid based: jeden cell - jedna jednotka, žádná větší jednotka, kolize by swapping 
        - neřešit koherenci, jednotky následující jednu cestu, vede k lining behavior 
        - počkat jednotkami, které se právě nemohou pohnout, případně ignoruj lokální kolize  

    - lokální řešení: steering skzre flocking behavior:
        - ne moc blízko ostatním
        - stejně rychle jako okolí, stejný směr jako okolí
        - cca ve středu okolí 

        - attractors: leader / střed / ...
        - repuslors: obstacles (globální look ahead), ...

        - emergentní: hard to debug 

- Steering: 
    - https://drive.google.com/file/d/1qmLmmIuYG4xGezSPfQfyMQE_8YiTs9Nc/view
    - kontrolování low-level pohybu 
    - čistě lokální, pracuje se sílamy měnícími se každý frame 
    - wander, avoid, arrive (slows before target), ... 

    - obecný algoritmus:
        - f = calc_f(...)
        - a = f/m 
        - a = clamp(a, maxAcc)
        - v += a * td 
        - p += v * td 
        - s = v.len 
        - look_dir = v.norm

        - position can be used for next-target-to-reach for anim. controller, speed for switching animations, ...
    - list of reynold steerings: 
        - seek and flee: steer to/from static target
            - to_target = target_pos - my_pos 
            - des_vel = normalize(to_target) * max_speed
            - steering = des_vel - velocity
        - pursue and evade 
        - wander: random steering
            -  at each step, random offset added to wander direction, offset constrained to be in cone/circle in front of the subject 
            - 
            - 
        - arrival: as seek but slow down as you approach it 
            - ramped_speed = max_speed * (distance / slowing_distance)
            - clipped_speed = min(ramped_speed, max_speed) 
            - des_vel = to_target * (clipped_speed / distance)
        - pursue & evade: seek and flee but target is dynamic
            - agent predicts location of the target next tick, the close the smaller lookahead 
        - ...

    - leader following, ... 
        - arrival with target slightly in front of/behind (záleží co chceme) leader + separation (kolize s ostatními)

    - path following: agent předpoví kde bude v dalším ticku dle dosavadní rychlosti, najde z toho bodu nejbližší bod cesty, steer k němu/ne pokud within margin 

- collision avoidance: 
    - lookahead + test na vzdálenost od středu obstacle pro rychlou implmentaci -> avoidance force od překážky 
        - problém více nebezpečí, pouze nejbližší je considered 

    - lookahead a steer away, problém u více agentů (sami mohou změnit kurz, ...), lepší nepředpokládat kooperaci (jinak velmi komplexní systém)

    - velocity obstracles: 
        - uhýbání dynamickým překážkám
        - pokud agent A ví o agentu B (lokace, velocity), můžeme detekovat velocity obstacle pro A: velocities které způsobí srážku 
        - analytické řešení 

    - v případě více agentů může vést k oscilujícím chováním (head-on trajektorije, ...)
        - řešení: RVO: předpokládá kooperaci, idea: každý agent si vyřeší svojí polovinu kolize, pokud oba úspěšně -> collision free 

- pathfinding
    - omezení: ne všechno se umí otočit na místě, ... 
    - různé start/end: 1:1, 1:N (jaký předmět), N:M (optimalizace)
    - těžko definovat co je dobrá cesta (krátká, bezpečná, vypadá přirozeně, ...) -> multi. optimalizace 

    - un/known environment, static/dynamic, ...

- pathdinding representace:
    - uniform tiles 
        - problém velkých otevřených prostor, pomalé
        - walkable/non-walkable tiles
        - smooth with funnel alg. 

        - zlepšeno skrze costs 
    - quad-tree tiles: 
        - navigace skrze středy 
        - divné cesty, vyžaduje smoothing (funnel)
        - zlepšení skzre frame of small nodes around each tile 
    - flow-fields 
        - obstacles: repulsors 
        - cíle: attractors 

- pathfinding algs
    - iterative deepening 
        - DFS s max depth, neotevře zas o tolik nodů víc než třeba, když stačí good enough cesta, může být dobré 
    - random walks
        - random, wall hugging, robust wall-hugging (než crossnes line mezi origin a cílem)
    - BFS / dijstra: otevírají se ty, co mají nejkratší cestu
    - Best first search: negarantuje, otevírají se ty, co se zdají nejblíž
    - A*: Ty co mají nejkratší cesta + permissible (menší než real) heuristika od cíle 
        - memory bound variace (drop worst node, backup value to parent)
        - u překážek otevře hodně nodes, jde řešit přes non-admissible heuristiku
        - hledání musí skončit před prvním krokem (není online), těžko funguje pro víc agentů 
    - JSP (jump point search)
        - funguje na regulárním gridu 
        - ignoruje symetrické cesty (sidesteps)
    - JSP+
        - Heuristika skrz pre-computed jump aheads prebaked (8-way vzdálenosti ke stěnám + jump points)
        - podrobně ve slidech https://drive.google.com/file/d/1oiHI_BUeSNmKUxvQT0eo8iS9iJr1hXHh/view
    - Goal bounding: 
        - neotevírat nodes skrz které nevede nejkratší cesta do finálního regionu (každá hrana má bounding box)
        - pouze statický environments, může být dost fast v maze-like prostředí
        - precomputation:
            - pro given node spust dijstra ve fill mode, mark each node reach z první edge 
            - spočítej bounding box -> result pro tu jednu hranu
        - approx floyd washall 4*edge instead of nodes^2

- adv pathfinding (ne u státnic)
    - bidirectional search
        - musí být smart zastavovací pravidlo (až po nalezení shortest_a+shortest_b > shortest_tog)
    - s chytrou heuristikou BA* 
        - alternativně paralelně 
    - D*: dynamický environent 
        - negarantuje nejkratší cestu
        - začíná s optimistickou mapou, pomalu upravuje as necessary 
    - Lifelong A*
        - při změně se přepočítají změněné cesty, otevřou relevantní vrcholy a pokračuje se 
    - RRT:
        - postupné vytváření tree-like representace neznámé mapy
        - random point, najde se nejbližší node, zkusí se z něj cesta, pokud existuje, tak se připojí
        - vylepšení: lokální okolí se přeskládá pro nejkratší možné cesty z source pokud cesty possible

Etologické motivace, modely populační dynamiky
- virtualní agenti: uvěřitelnost, hry, empatie
- animati: hlavně výpočet, musí být plausibilní datově, ...

- analytické modely
    - na základě empirických dat, zkoumání např. populací živých organismů
    - zpravidla diferenciální rovnice
    - většinou příliš obecné 

    - např: N(t+dt) = N(t) + b*N(t)*dt - d*N(t)*dt // d: deaths, b: births 
    - složitější modely predátor kořist, hostitel parazit, ... 
    
    - predátor x kořist
        - dx = ax - bxy | ax: kořist se množí, -bxy je zabíjena predátorem
        - dy = -cy + dxy | -cy: umírají, +dxy: množení když je dost jídla
    - populační křivky: cykly populačního grafu (množství x, y bude cyklovat; stabilně nebo ne)

- simulační modely
    - hodně parametrů, ...

- hydraulic model: 
    - https://www.youtube.com/watch?v=AnXydNDVlss
    - Lorenze's model of motivation

    - fixed action pattern: all members within species, clockwork 
        - aktivují se při přítomnosti external stimuli: releaser 
        - větší interní motivace -> spíš reakce na slabší releaser 

    - motivational energy: action specific energy: akumulace vody v rezervoáru
    - innate releasing mechanism: podle velikosti externího stimulu se vypustí množství vody dostatečné na naplnění druhého semi-rezerováru
        - dle úrovně hladiny se spustí jedna z několika možstvích akcí :: fixed action pattern
    - behavioral quitness: po aktivaci fixed action pattern chvíli trvá, než stejný stimulus způsobí stejnou akci (naplní se rezervoár)

    - male: has fixed patterns
    - female: releser for male's courtship fixed action pattern  

    - the more motivated we are, the more responsive we are to stimuli 
    - hromadí se energie, ta časem spustí akci 

- free flow hierarchy
    - http://www.dgp.toronto.edu/~tu/thesis/node165.html
    - node hierarchie, all nodes express preference for set of motor action, finální roznodnutí weighted sum 
    - propagace takovéto hierarchie skze několik vrstev, nakonec se vybere akce argmax 
    - no focus attention
    - hierarchie senzorických inputů / vnitřních drivů, ...
    - těžko správně nastavit/naučit váhy, hodně parametrů

    - ~podobné dnešním DNN

Metody pro učení agentů; zpětnovazební učení, základní formy učení zvířat
- s učitelem: máme (xk, yk) pro k=0..N, chceme f(xi) = f(yi)
- bez učitele: máme xk, chceme P(X=xi) (clustering, anonimaly detection, EM algoritmus, ...)
- semisupervised: máme část labled, část ne
- zpětně-vazební: máme s0a1s1a2, po epizodě dostaneme signál, chceme sadu pravidel pro maximalizaci odměn 

- supervised learning: 
    - Trénovací, testovací množina
    - Mnoho různých způsobů učení podle domény dat 

- reinfrorcement learning (AI2 notes :: #11): 
    - explorace vs. explotace: zkoušení nových strategií vs. zlepšování slušných již nalezených
        - s určitou p vyber náhodnou akci regardless learned stuff 
        - vybírej vždycky argmax / podle distribuce (při učení) 

    - pasivní učení: pevně daná strategie, učí se ohodnocení jednotlivych stavů, učí se utility func: U(s)
        - většinou se musíš aktivně učit U(s)->R, a přechodovou funkci T(s)->s (frekvenční tabulka)
        - ADP (adaptive dynamic programming)
            - iteratively learns transition & rewards model 
            - adjust state's utility to agree with all of the successors given transition 
            - U(s) = U(s) + a(R(s) + y~U(s') - U(s)) | ~U(s') střední hodnota successorů skrze learned transition 
        - Temporal difference: 
            - neučí se transition model 
            - U(s) = U(s) + a(R(s) + yU(s') - U(s)) | U(s') ten konkrétní vybranej transition, faster 
            - 
    - aktivní učení: učí se policy: Pi(s, a)
        - Monte carlo search: U(s) <- R(s) + y*max_a f(SUM_s'( P(s'|s,a) * U(s') ), N(s,a))
            - passive agent and using value or policy iteration to learn optimal choices 
                - exploration function f makes it an active agent  

            - N(s, a): number of times action a has been tried in s
            - U(s): optimistic estimate of utility
            - f(u, n) exploration function, defines tradeoff between exploration vs explotation, bigger with higher u, lower with higher n

            - propagates the benefits of exploration back so that paths towards unexplored regions are weighted more highly
                - reason why there's the optimistic estimate of utility on the right side
    
        - for active temporal difference learning agent there's a need for transition model 
            - To select the best action for current state (only have utility func. for states)
            - Needs to learn the transition model as well

        - Q learning
            - Q(s, a) denotes value for doing an action a in a state s
            - U(s) = max_a(s, a)

            - TD agent learns with Q function doesn't need explicit transition model P(s' |s, a)
            - At equilibrium: Q(s, a) = R(s) + ySUM_s'( P(s'|s, a) * max_a'Q(s', a') )
                - Bellman: Q(st, at) = E[Rt+1 + y*Rt+2 + y^2*Rt+3... | st, at], leads to ^^
                - Convergence: 
                    - |BUi - BU'i| <= y|Ui - U'i| // The probabilities in exp. val for updates and Rewards are the same for U and U'
                    - |BUi - U| <= y|Ui - U| // U: vector of true utilities -> fixed point
            - Update: Q(s, a) <- Q(s, a) + alpha * ( R(s) + y * max_a'Q(s', a') - Q(s,a) )
                - whenever action a is executed for state s leading to state s'

            - Deep learning verze:
                - baseline network implementující U(s), pro lepší rozlišení stavů
                - experience buffer 
        - SARSA
            - similar to Q-learning 
            - instead of maxing over 'a in update it updates after 2 actions & uses true a, a'

        - both SARSA and Q-learning slower than ADP agent, local updates don't force consistency
        - both converge to optimal policy given enough time

        - model free methods (e.g. Q-learning) don't need knowledge base for environment modeling
        - for more complex environments knowledge based approach with explicit trainsition model can be better (ADP)

    - REINFORC algoritmus
        - Nuronová síť implementuje policy (state->action), podle odměny připočteme/odečteme gradient pro vybranou akci  
        - střídání dvou sítí, jedna se učí, druhá pro aktuální policy, experience buffer, ...

    - MAS setting:
        - beru je jako jeden meta-agent, učím přímo Q funkci pro tuple stavů a akcí 
        - self-play, každej agent se učí separátně, všichni ale mohou být instance jednoho 

- Formy učení zvířat: 
    - chaining 
        - process spojení série malých chování do dlouhé sekvence 
        - na konci sekvence je chování odměněno reinforcementem 

        - backward chaining: 
            - zvíře se naučí poslední (odměměné) chování jako první, postupně se přidá to předtím, ...
        - forward chaining: 
            - učíme předně první, pak druhé, ... 
            - většinou horší pro dlouhé sekvence u zvířat 
            - přirozenější (prý), snadnější opustit v půlce
        - https://thesensorytoolbox.com/forward-chaining-vs-backward-chaining/

    - shaping 
        - odměňování sub-stepů v složitějším úkolu 
        - postupně ho dovedeme do chování, které actually chceme 

        - The differential reinforcement of successive approximations toward a target behavior
            - differential: nesmím ocenit všechno 
            - approximations: postupně zvyšuju cíle, ... 

    - Habituace 
        - úbytek reakce na opakování téhož podnětu (přitom je reakce na jiné podněty zachována - nejde tedy o únavu) 
        - když na chování je reakce co má minimální dopad na zvíře, tak to chování vymizí (třeba zvířata co si zvyknou, že lidi nejsou predátoři)
    - Senzitizace 
        - nárůst reakce na určitý podnět (opak habituace) 
        - když zvíře má silný zážitek, tak si s chováním asociuje tuhle emoci (ať už špatnou nebo dobrou)
    - Imprinting 
        - proces učení vázaný k určité fázi vývoje jedince, který vede k trvalým a nezvratným změnám chování (kachny a Konrad Lorenz) 
        - první věc co právě narozené zvíře vidí si imprintuje jako matku
    - Klasické podmiňování 
        - Pavlov a jeho psi. 
        - Nepodmíněný podnět - sám vyvolává reakci, 
        - podmíněný podnět - vyvolává reakci až když je spojen s nepodmíněným podnětem. 
        - Když psi dostávají jídlo a zároveň zvoníme zvonkem, časem si asociujou, že když zvoníme zvonkem tak dostanou jídlo.
    - Operantní podmiňování 
        - Chování zvířete upevňujeme nebo oslabujeme za pomocí stimulací jako následků jeho chování. 
        - Zároveň používáme stimulus jako předchůdce chování - trigger pro chování (povel sedni). Typy následků: pozitivní a negativní posilování (reinforcement) a tresty. 
        - Pozitivní posilování chování je nejtypičtější - jednodušše odměníme zvíře když dělá to co chceme. 
        - Negativní posilování funguje tak, že v prostředí je konstantně nějaký nepříjemný vjem a když zvíře dělá to co chceme tak se vjem odstraní. Rozdíl oproti trestu je jasný, trest zvířeti dá nový negativní vjem, chování oslabuje. Pozn. Chaining - učíme sled chování, jakože psa chceme naučit, aby sedl, lehl, zatancoval a zaštěkal. 
        - Princip je ten, že psovi dáváme sérii povelů a odměňujeme ho až nakonec. (reinforcement learning)
